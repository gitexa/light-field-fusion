{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import alpha_composite\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn principle the pipeline should contain the following functions:\\n\\n1. Depth map to Zero-one depth tensor\\nInput :  (512,512) depth map\\nOutput:  (512,512,d) depth tensor\\nImplementation: \\n\\n2. Zero-One depth tensor to smoothed depth tensor\\nInput :  (512,512,d) depth tensor\\nOutput:  (512,512,d) depth tensor\\nImplementation: naive_alpha\\n\\n3. Depth tensor and image to RGBA tensor\\nInput :  (512,512,d) depth tensor\\n         (3,512,512) RGB image\\nOutput:  (4,512,512,d) RGBA tensor\\nImplementation: rgba\\n\\n4. RGBA tensor homography warp to target pose\\nInput :  (4,512,512,d) RGBA tensor\\n          Target Pose\\nOutput:  (4,512,512,d) RGBA tensor warped to target pose\\nImplementation: MISSING\\n\\n5. RGBA tensor alpha composite to RGB image\\nInput :  (4,512,512,d) RGBA tensor\\nOutput:  (4,512,512) RGBA image, \\nImplementation: back_to_front_alpha_composite\\n\\n6. RGB images + alphas to blended output RGB image (Equation 8)\\nInput :  5 (4,512,512) RGBA images\\nOutput:  (3,512,512) RGB image\\nImplementation: \\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In principle the pipeline should contain the following functions:\n",
    "\n",
    "1. Depth map to Zero-one depth tensor\n",
    "Input :  (512,512) depth map\n",
    "Output:  (512,512,d) depth tensor\n",
    "Implementation: create_depth_tensor\n",
    "\n",
    "2. Zero-One depth tensor to smoothed depth tensor\n",
    "Input :  (512,512,d) depth tensor\n",
    "Output:  (512,512,d) depth tensor\n",
    "Implementation: naive_alpha\n",
    "\n",
    "3. Depth tensor and image to RGBA tensor\n",
    "Input :  (512,512,d) depth tensor\n",
    "         (3,512,512) RGB image\n",
    "Output:  (4,512,512,d) RGBA tensor\n",
    "Implementation: rgba\n",
    "\n",
    "4. RGBA tensor homography warp to target pose\n",
    "Input :  (4,512,512,d) RGBA tensor\n",
    "          Target Pose\n",
    "Output:  (4,512,512,d) RGBA tensor warped to target pose\n",
    "Implementation: MISSING\n",
    "\n",
    "5. RGBA tensor alpha composite to RGB image\n",
    "Input :  (4,512,512,d) RGBA tensor\n",
    "Output:  (4,512,512) RGBA image, \n",
    "Implementation: back_to_front_alpha_composite\n",
    "\n",
    "6. RGB images + alphas to blended output RGB image (Equation 8)\n",
    "Input :  5 (4,512,512) RGBA images\n",
    "Output:  (3,512,512) RGB image\n",
    "Implementation: \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input  : (512,512) depth_map\n",
    "#          Integer layers          \n",
    "# Output : (512,512, layers) depth tensor\n",
    "\n",
    "def create_depth_tensor(depth_map, layers):\n",
    "    assert depth_map.shape == (512,512), 'Depth map needs to be shape (512,512)'\n",
    "    assert type(layers) == int, 'layers needs to be integer'\n",
    "    \n",
    "    #get the min_depth and the max_depth\n",
    "    min_depth = torch.min(depth_map)\n",
    "    max_depth = torch.max(depth_map)*1.000001\n",
    "    \n",
    "    #shift all depths by the min_depth\n",
    "    depth_rel = depth_map-min_depth\n",
    "    \n",
    "    #get the bin_size\n",
    "    bin_size  = (max_depth-min_depth)/layers\n",
    "    \n",
    "    #get the depth_layer for every pixel\n",
    "    depth_layers = (depth_rel/bin_size).int()\n",
    "    \n",
    "    #convert to 3d 1-hot-encoding depth_tensor (512,512,layers)\n",
    "    depth_tensor = (torch.arange(layers) == depth_layers[...,None]).int()\n",
    "    \n",
    "    return depth_layers, depth_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512, 10])\n",
      "Testpixel 0\n",
      "Depth map value: tensor(0.3427)\n",
      "Depth layer: tensor(3, dtype=torch.int32)\n",
      "Depth tensor: tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "---------------------------\n",
      "Testpixel 1\n",
      "Depth map value: tensor(0.0210)\n",
      "Depth layer: tensor(0, dtype=torch.int32)\n",
      "Depth tensor: tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "---------------------------\n",
      "Testpixel 2\n",
      "Depth map value: tensor(0.2487)\n",
      "Depth layer: tensor(2, dtype=torch.int32)\n",
      "Depth tensor: tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "---------------------------\n",
      "Testpixel 3\n",
      "Depth map value: tensor(0.9582)\n",
      "Depth layer: tensor(9, dtype=torch.int32)\n",
      "Depth tensor: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=torch.int32)\n",
      "---------------------------\n",
      "Testpixel 4\n",
      "Depth map value: tensor(0.0380)\n",
      "Depth layer: tensor(0, dtype=torch.int32)\n",
      "Depth tensor: tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "---------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function create_depth_tensor\n",
    "\n",
    "depth_map = torch.rand((512,512))\n",
    "layers = 10\n",
    "\n",
    "layer, tensor = create_depth_tensor(depth_map, layers)\n",
    "print(tensor.shape)\n",
    "for i in range(5):\n",
    "    pixel = torch.randint(0,512,size=(2,))\n",
    "    print('Testpixel',i)\n",
    "    print('Depth map value:',depth_map[pixel[0],pixel[1]])\n",
    "    print('Depth layer:',layer[pixel[0],pixel[1]])\n",
    "    print('Depth tensor:',tensor[pixel[0],pixel[1]])\n",
    "    print('---------------------------')\n",
    "    \n",
    "    \n",
    "# Test all pixels\n",
    "a=torch.argmax (tensor, dim=2)\n",
    "#torch.allclose( a, layer)\n",
    "b=layer - a\n",
    "for i in range(512):\n",
    "    for j in range(512):\n",
    "        if b[i,j] != 0:\n",
    "            print(i,j)\n",
    "            print(depth_map[i,j])\n",
    "            print(layer[i,j])\n",
    "            print(a[i,j])\n",
    "            print(tensor[i,j])\n",
    "b[b != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a depth tensor of shape (512,512,depth) as input\n",
    "# input is assumed to assign every pixel of the (512,512) image to exactly one depth\n",
    "# output is a (512,512,depth) tensor with smoothed depth information\n",
    "# smoothes depth channel with filter [0.1 , 0.2 , 0.4 , 0.2 , 0.1]\n",
    "\n",
    "def naive_alpha(depth_tensor):\n",
    "    \n",
    "    assert len(depth_tensor.shape)==3, 'Depth tensor needs to be 512x512xdepth'\n",
    "    depth = depth_tensor.shape[2]\n",
    "    assert depth_tensor.shape == (512,512,depth), 'Depth tensor needs to be 512x512xdepth'\n",
    "    assert torch.allclose(torch.sum(depth_tensor, dim=2), torch.ones((512,512))), 'Depth tensor not normalized'\n",
    "    \n",
    "    # Note: Add 4 units of 0-padding in depth direction for the boundary conditions\n",
    "    zero_padding = torch.zeros((512,512,2))\n",
    "    alpha        = torch.cat((zero_padding, depth_tensor, zero_padding), dim=2)\n",
    "    \n",
    "    # Naively distribute the alpha=1 value over its neighborhood\n",
    "    # Equivalent to a 1D-convolution of the depth dimension with the filter [0.1 , 0.2 , 0.4 , 0.2 , 0.1]  ??\n",
    "    alpha = alpha[:,:,0:-4]*0.1 + alpha[:,:,1:-3]*0.2 + alpha[:,:,2:-2]*0.4 + alpha[:,:,3:-1]*0.2 + alpha[:,:,4:]*0.1\n",
    "\n",
    "    \n",
    "    # Assure normalization\n",
    "    # Note: In case the significant depth is at the boundary, this is necessary ??\n",
    "    sum_along_depth = torch.sum(alpha, dim=2)\n",
    "    alpha           = alpha / sum_along_depth.unsqueeze(2)\n",
    "    \n",
    "    assert alpha.shape == (512,512,depth), 'Alpha Dimensionality failed'\n",
    "    assert torch.allclose(torch.sum(alpha, dim=2), torch.ones((512,512))), 'Alpha Normalization failed'\n",
    "\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "\n",
    "#ToDo\n",
    "       \n",
    "# Boundary conditions: Something else than 0-padding?\n",
    "    # mpi[:,:,0]  =  mpi[:,:,0]  * 0.5  + mpi[:,:,1]  * 0.5\n",
    "    # mpi[:,:,1]  =  mpi[:,:,0]  * 0.25 + mpi[:,:,1]  * 0.5 + mpi[:,:,2]  * 0.25\n",
    "    \n",
    "    # mpi[:,:,-1] =  mpi[:,:,-1] * 0.5  + mpi[:,:,-2] * 0.5\n",
    "    # mpi[:,:,-2] =  mpi[:,:,-1] * 0.25 + mpi[:,:,-2] * 0.5 + mpi[:,:,-3] * 0.25 \n",
    "\n",
    "    \n",
    "# Image size (512,512) to arbitrary\n",
    "\n",
    "\n",
    "# Smoothing kernel as input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the naive_alpha function for depth=16 \n",
    "\n",
    "one_layer = torch.ones((512,512,1))\n",
    "zero_layer= torch.zeros((512,512,1))\n",
    "\n",
    "zero_6layer = torch.zeros((512,512,6))\n",
    "zero_9layer = torch.zeros((512,512,9))\n",
    "zero_14layer= torch.zeros((512,512,14))\n",
    "zero_15layer= torch.zeros((512,512,15))\n",
    "\n",
    "# Test having the significant pixel at different depths to see boundary condition behaviour\n",
    "test_0 = torch.cat((one_layer, zero_15layer) , dim=2)\n",
    "test_1 = torch.cat((zero_layer, one_layer, zero_14layer), dim=2)\n",
    "test_7 = torch.cat((zero_6layer, one_layer, zero_9layer), dim=2)\n",
    "test_14= torch.cat((zero_14layer,one_layer , zero_layer), dim=2)\n",
    "test_15= torch.cat((zero_15layer, one_layer ), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5714, 0.2857, 0.1429, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_alpha(test_0)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2222, 0.4444, 0.2222, 0.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_alpha(test_1)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.2000, 0.4000, 0.2000, 0.1000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_alpha(test_7)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1111, 0.2222, 0.4444, 0.2222])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_alpha(test_14)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.1429, 0.2857, 0.5714])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_alpha(test_15)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgba(image, depth_tensor):\n",
    "    \n",
    "    # Input: RGB image 3 x 512 x 512 \n",
    "    # Input: Depth tensor 512 x 512 x Depth\n",
    "    # Output: RGB-alpha depth image 4 x 512 x 512 x Depth\n",
    "    \n",
    "    assert len(depth_tensor.shape)==3, 'Depth tensor needs to be 512x512xdepth'\n",
    "    Depth = depth_tensor.shape[2]\n",
    "    assert depth_tensor.shape == (512,512,Depth), 'Depth tensor needs to be 512x512xdepth'\n",
    "    assert torch.allclose(torch.sum(depth_tensor, dim=2), torch.ones((512,512))), 'Depth tensor not normalized'\n",
    "    \n",
    "    assert len(image.shape)==3, 'Image must be 3x512x512'\n",
    "    assert image.shape[0]==3, 'Image must be 3x512x512'\n",
    "\n",
    "    \n",
    "    output=torch.zeros((4,512,512,Depth))\n",
    "    \n",
    "    for d in range(Depth):\n",
    "                \n",
    "        output[:,:,:,d]=torch.stack([image[0,:,:],image[1,:,:],image[2,:,:],depth_tensor[:,:,d]], dim=0)\n",
    "                \n",
    "    return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random pixel1 : tensor([234, 115])\n",
      "RGB values of test image at pixel1 : tensor([0.1573, 0.4703, 0.9468])\n",
      "Alpha values along depth at pixel1 : tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "RGBa value at pixel1 at depth0 : tensor([0.1573, 0.4703, 0.9468, 0.0000])\n",
      "RGBa value at pixel1 at depth6 : tensor([0.1573, 0.4703, 0.9468, 1.0000])\n",
      "RGBA shape: torch.Size([4, 512, 512, 16])\n",
      "----------------------------------------\n",
      "Random pixel2 : tensor([197,  86])\n",
      "RGB values of test image at pixel2 : tensor([0.5164, 0.9373, 0.7269])\n",
      "Alpha values along depth at pixel2 : tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "RGBa value at pixel2 at depth0 : tensor([0.5164, 0.9373, 0.7269, 0.0000])\n",
      "RGBa value at pixel2 at depth6 : tensor([0.5164, 0.9373, 0.7269, 1.0000])\n",
      "RGBA shape: torch.Size([4, 512, 512, 16])\n"
     ]
    }
   ],
   "source": [
    "# Test the rgba implementation with a random image\n",
    "# test_depth tensor has one layer depth=6 where alpa=1, alpha=0 everywhere else\n",
    "\n",
    "test_image = torch.rand((3,512,512))\n",
    "\n",
    "test_depth_tensor = torch.cat((zero_6layer, one_layer, zero_9layer), dim=2)\n",
    "\n",
    "test_rgba = rgba(test_image, test_depth_tensor)\n",
    "\n",
    "# Look at test results for random pixels\n",
    "pixel1 = torch.randint(0, 255, size=(2,))\n",
    "print('Random pixel1 :', pixel1)\n",
    "print('RGB values of test image at pixel1 :', test_image[:,pixel1[0],pixel1[1]])\n",
    "print('Alpha values along depth at pixel1 :', test_depth_tensor[pixel1[0],pixel1[1]])\n",
    "print('RGBa value at pixel1 at depth0 :', test_rgba[:,pixel1[0],pixel1[1],0])\n",
    "print('RGBa value at pixel1 at depth6 :', test_rgba[:,pixel1[0],pixel1[1],6])\n",
    "print('RGBA shape:', test_rgba.shape)\n",
    "\n",
    "print('----------------------------------------')\n",
    "\n",
    "pixel2 = torch.randint(0, 255, size=(2,))\n",
    "print('Random pixel2 :', pixel2)\n",
    "print('RGB values of test image at pixel2 :', test_image[:,pixel2[0],pixel2[1]])\n",
    "print('Alpha values along depth at pixel2 :', test_depth_tensor[pixel2[0],pixel2[1]])\n",
    "print('RGBa value at pixel2 at depth0 :', test_rgba[:,pixel2[0],pixel2[1],0])\n",
    "print('RGBa value at pixel2 at depth6 :', test_rgba[:,pixel2[0],pixel2[1],6])\n",
    "print('RGBA shape:', test_rgba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random pixel1 : tensor([143, 124])\n",
      "RGB values of test image at pixel1 : tensor([0.2739, 0.2924, 0.7183])\n",
      "Alpha values along depth at pixel1 : tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "RGBa value at pixel1 at depth0 : tensor([0.2739, 0.2924, 0.7183, 0.0000])\n",
      "RGBa value at pixel1 at depth4 : tensor([0.2739, 0.2924, 0.7183, 0.0000])\n",
      "RGBa value at pixel1 at depth6 : tensor([0.2739, 0.2924, 0.7183, 1.0000])\n",
      "RGBA shape: torch.Size([4, 512, 512, 16])\n",
      "----------------------------------------\n",
      "Random pixel2 : tensor([206,  37])\n",
      "RGB values of test image at pixel2 : tensor([0.5064, 0.9441, 0.9367])\n",
      "Alpha values along depth at pixel2 : tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "RGBa value at pixel2 at depth0 : tensor([0.5064, 0.9441, 0.9367, 0.0000])\n",
      "RGBa value at pixel2 at depth4 : tensor([0.5064, 0.9441, 0.9367, 0.0000])\n",
      "RGBa value at pixel2 at depth6 : tensor([0.5064, 0.9441, 0.9367, 1.0000])\n",
      "RGBA shape: torch.Size([4, 512, 512, 16])\n"
     ]
    }
   ],
   "source": [
    "# Test the rgba implementation with a random image\n",
    "# test_depth_tensor2 is the naive_alpha output of test_depth_tensor \n",
    "\n",
    "test_image2 = torch.rand((3,512,512))\n",
    "\n",
    "test_depth_tensor2 = naive_alpha(test_depth_tensor)\n",
    "\n",
    "test_rgba2 = rgba(test_image2, test_depth_tensor2)\n",
    "\n",
    "# Look at test results for random pixels\n",
    "pixel1 = torch.randint(0, 255, size=(2,))\n",
    "print('Random pixel1 :', pixel1)\n",
    "print('RGB values of test image at pixel1 :', test_image[:,pixel1[0],pixel1[1]])\n",
    "print('Alpha values along depth at pixel1 :', test_depth_tensor[pixel1[0],pixel1[1]])\n",
    "print('RGBa value at pixel1 at depth0 :', test_rgba[:,pixel1[0],pixel1[1],0])\n",
    "print('RGBa value at pixel1 at depth4 :', test_rgba[:,pixel1[0],pixel1[1],4])\n",
    "print('RGBa value at pixel1 at depth6 :', test_rgba[:,pixel1[0],pixel1[1],6])\n",
    "print('RGBA shape:', test_rgba.shape)\n",
    "\n",
    "print('----------------------------------------')\n",
    "\n",
    "pixel2 = torch.randint(0, 255, size=(2,))\n",
    "print('Random pixel2 :', pixel2)\n",
    "print('RGB values of test image at pixel2 :', test_image[:,pixel2[0],pixel2[1]])\n",
    "print('Alpha values along depth at pixel2 :', test_depth_tensor[pixel2[0],pixel2[1]])\n",
    "print('RGBa value at pixel2 at depth0 :', test_rgba[:,pixel2[0],pixel2[1],0])\n",
    "print('RGBa value at pixel2 at depth4 :', test_rgba[:,pixel2[0],pixel2[1],4])\n",
    "print('RGBa value at pixel2 at depth6 :', test_rgba[:,pixel2[0],pixel2[1],6])\n",
    "print('RGBA shape:', test_rgba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to alpha_composite a 4d rgb-alpha depth image from back to front\n",
    "# Input  (4,512,512,Depth)\n",
    "# Output (4,512,512)\n",
    "\n",
    "def back_to_front_alphacomposite(rgba_depth_image):\n",
    "    \n",
    "    assert len(rgba_depth_image.shape)==4 , 'Input image needs to have shape 4 x 512 x 512 x Depth'\n",
    "    assert rgba_depth_image.shape[0] == 4 , 'Input image needs to have shape 4 x 512 x 512 x Depth'\n",
    "    Depth = rgba_depth_image.shape[3] \n",
    "    \n",
    "    img = transforms.ToPILImage('RGBA')(rgba_depth_image[:,:,:,-1])\n",
    "\n",
    "    for d in reversed(range(1,Depth)):\n",
    "        \n",
    "        layer = rgba_depth_image[:,:,:,d-1]\n",
    "        layer = transforms.ToPILImage('RGBA')(layer)\n",
    "        \n",
    "        img = alpha_composite( layer , img)\n",
    "\n",
    "    img = transforms.ToTensor()(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7324, 0.7324, 0.7324, 0.7324, 0.7324, 0.7324, 0.7324, 0.7324, 0.7324,\n",
      "         0.7324, 0.7324, 0.7324, 0.7324, 0.7324, 0.7324, 0.7324],\n",
      "        [0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220,\n",
      "         0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220, 0.0220],\n",
      "        [0.9619, 0.9619, 0.9619, 0.9619, 0.9619, 0.9619, 0.9619, 0.9619, 0.9619,\n",
      "         0.9619, 0.9619, 0.9619, 0.9619, 0.9619, 0.9619, 0.9619],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.2000, 0.4000, 0.2000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([0.7294, 0.0196, 0.9608, 0.6902])\n"
     ]
    }
   ],
   "source": [
    "a=back_to_front_alphacomposite(test_rgba2)\n",
    "print(test_rgba2[:,0,0])\n",
    "print(a[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
